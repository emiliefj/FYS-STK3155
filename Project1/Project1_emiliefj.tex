

\documentclass[%
oneside,                 % oneside: electronic viewing, twoside: printing
final,                   % draft: marks overfull hboxes, figures with paths
11pt,
titlepage,
english]{article}

\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc, url}
\usepackage{textcomp}
\usepackage{amsmath, amssymb}
\usepackage{amsbsy, amsfonts}
\usepackage{graphicx, color}
\usepackage{parskip}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{url}
\usepackage{flafter}


%
% Marger
%
\usepackage{geometry}
\geometry{headheight=0.01mm}
\geometry{top=24mm, bottom=30mm, left=39mm, right=39mm}


%
% Formattering av Python listings
%
\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
\usepackage{listings}
\lstset{language=python,
basicstyle=\ttfamily\footnotesize,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
% numbers=left,
% numberstyle=\tiny\color{black},
stepnumber=2,
numbersep=10pt,
tabsize=2,
showspaces=false,
showstringspaces=false,
frame= single,
breaklines=true}

%
% Gjør tabeller mer luftige
%
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{10pt}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

%
% Hyperlinks in PDF:
%
\definecolor{linkcolor}{rgb}{0,0,0.4}
\usepackage{hyperref}
\hypersetup{
	breaklinks=true,
	colorlinks=true,
	linkcolor=linkcolor,
	urlcolor=linkcolor,
	citecolor=black,
	filecolor=black,
	%filecolor=blue,
	pdfmenubar=true,
	pdftoolbar=true,
	%bookmarksdepth=3   % Uncomment (and tweak) for PDF bookmarks with more levels than the TOC
}
%\hyperbaseurl{}   % hyperlinks are relative to this root


%
% navn og tittel
%
\author{Emilie Fjørner}
\title{Project 1 \\ FYS-STK3155}


\begin{document}
	
	\maketitle
	
	\section*{Abstract}
	
	\subsection*{a)}
	
	\section*{Introduction}
	% Note: teken from assignment, rewrite:
	The main aim of this project is to study in more detail various
	regression methods, including the Ordinary Least Squares (OLS) method,
	Ridge regression and finally Lasso regression. 
	
	The methods are in turn combined with resampling techniques like the bootstrap method and cross validation. 
	
	We will first study how to fit polynomials to a specific
	two-dimensional function called \href{{http://www.dtic.mil/dtic/tr/fulltext/u2/a081688.pdf}}{Franke's function}.  
	This is a function which has been widely used when testing various
	interpolation and fitting algorithms. Furthermore, after having
	established the model and the method, we will employ resampling
	techniques such as cross-validation and/or bootstrap in order to perform a
	proper assessment of our models. We will also study in detail the
	so-called Bias-Variance trade off.
	
	
	The Franke function, which is a weighted sum of four exponentials  reads as follows
	\begin{align*}
		f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
		&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
	\end{align*}
	
	The function will be defined for $x,y\in [0,1]$.  Our first step will
	be to perform an OLS regression analysis of this function, trying out
	a polynomial fit with an $x$ and $y$ dependence of the form $[x, y,
	x^2, y^2, xy, \dots]$. We will also include bootstrap first as
	a resampling technique.  After that we will include cross-validation. As in homeworks 1 and 2, we can use a uniform
	distribution to set up the arrays of values for $x$ and $y$, or as in
	the example below just a set of fixed 
	values for $x$ and $y$ with a given step
	size.  We will fit a
	function (for example a polynomial) of $x$ and $y$.  Thereafter we
	will repeat much of the same procedure using the Ridge and Lasso
	regression methods, introducing thus a dependence on the bias
	(penalty) $\lambda$.
	
	Finally we are going to use (real) digital terrain data and try to
	reproduce these data using the same methods. We will also try to go
	beyond the second-order polynomials mentioned above and explore 
	which polynomial fits the data best.
	
	\section*{Methods}
	
	\section*{Results}
	
	\section*{Discussion}
	
	\section*{Conclusions and Perspectives}
	
	\section*{Appendix}
	
	\section*{Bibliography}
	
\end{document}