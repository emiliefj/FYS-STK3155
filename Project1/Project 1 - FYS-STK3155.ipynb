{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this first project of the course we are looking at linear regression and resampling methods. The goal is to implement three methods for linear regression; Ordinary Least Squares, Ridge, and Lasso regression, and study their performance and behavior. Two types of resampling methods, namely the bootstrap and k-fold cross validation is used to better evaluate the method and to determine the optimal value for the relevent parameters.\n",
    "\n",
    "We study two types of data. First we create synthetic data using the Franke function. This data is then used to explore and verify the models. After this we move on to real data, and in this project we will look at map data from [UCSG's EarthExplorer](https://earthexplorer.usgs.gov/), more specifically elevation. We explore how the models perform on this data\n",
    "\n",
    "We will look at the performance of the models and study their biance variance trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "We create synthetic data using the Franke function, as well as real digital terrain data to explore our regression models.\n",
    "## The Franke function \n",
    "The Franke function is a two dimensional weighted sum of four exponentials. It has two Gaussian peaks of different heights, and a smaller dip and is often used as a test function in interpolation problems.\n",
    "\n",
    "The function is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f(x,y) &= \\frac{3}{4}\\exp{\\left(-\\frac{(9x-2)^2}{4} - \\frac{(9y-2)^2}{4}\\right)}+\\frac{3}{4}\\exp{\\left(-\\frac{(9x+1)^2}{49}- \\frac{(9y+1)}{10}\\right)} \\\\\n",
    "&+\\frac{1}{2}\\exp{\\left(-\\frac{(9x-7)^2}{4} - \\frac{(9y-3)^2}{4}\\right)} -\\frac{1}{5}\\exp{\\left(-(9x-4)^2 - (9y-7)^2\\right) }.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and will be defined for $x,y\\in [0,1]$. See figure (?) for a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data\n",
    "We will use topological map data as real data for trying out our regression methods. I used EarthExplorer[1] to find a suitable map of elevation and chose an area over the Teton mountain range in Wyoming, USA. The map section had the enitityId SRTM1N43W111V3. I downloaded it as a GeoTIFF file with resolution of 1 arc second. A plot of this map is shown in figure (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "I explore three different methods for linear regression, as well as two methods for resampling. \n",
    "## Regression Methods\n",
    "\n",
    "### OLS\n",
    "Ordinary Least Squares Regression (OLS) fits a linear model with coefficients $\\beta_i$ to minimize the residual sum of squares between the output value (aka dependent or target variable) in the dataset, and the output as predicted by the linear approximation. With $\\boldsymbol{X}$ as a matrix of the input variables, and $\\boldsymbol{y}$ as the output or target, we approximate the target as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}}= \\boldsymbol{X}\\boldsymbol{\\beta},\n",
    "$$\n",
    "\n",
    "So the goal of OLS is to find the optimal $\\boldsymbol{\\hat{\\beta}}$ that minimizes the difference between the values $\\boldsymbol{\\hat{y_i}}$ and $\\boldsymbol{y_i}$. \n",
    "\n",
    "Defining the loss function to quantify  this difference, or spread, as:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2=\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{\\hat{y}}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{\\hat{y}}\\right)\\right\\},\n",
    "$$\n",
    "\n",
    "We want to minimize this function, and by taking the derivative of $L$ with respect to the individual $\\boldsymbol{\\beta_j}$ and solving for $\\boldsymbol{\\beta}$ we find the solution \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} =\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "that can be used to calculate $\\boldsymbol{\\hat{\\beta}}$.\n",
    "\n",
    "From an new input $\\boldsymbol{X_{a}}$ we can use the found $\\boldsymbol{\\hat{\\beta}}$ to calculate an estimate or prediction for the target $\\boldsymbol{y_a}$, $\\boldsymbol{\\hat{y_a}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "Ridge regression is a modification of OLS which puts a restriction on the size of the individual coefficients $\\boldsymbol{\\beta}$. This is particularly useful in models with many, partly correlated, input values. The coefficients are then likely to become poorly determined, and they tend to have high variance.\n",
    "\n",
    "To combat this behavior ridge regression adds a penalty term to the loss function from the OLS model penalizing large beta values. The penalty is equivalent to the square of the magnitude of the coefficients. More succintly the ridge model adds L2 regularization to the OLS model.\n",
    "\n",
    "starting with the expression from the above section, \n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta})=\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2=\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{ij}\\beta_j^2)^2,\n",
    "$$\n",
    "\n",
    "a penalty term is added\n",
    "$$\n",
    "L(\\boldsymbol{\\beta})=\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\sum_{j=1}^{p}\\beta_j^2.\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{p} \\beta_i^2 \\leq t,\n",
    "$$\n",
    "\n",
    "where $t$ is a positive number.\n",
    "\n",
    "In matrix notation\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{X},\\boldsymbol{\\beta})=\\frac{1}{n}\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})\\right\\}+\\lambda\\boldsymbol{\\beta}^T\\boldsymbol{\\beta},\n",
    "$$\n",
    "\n",
    "From which we get an expression for the coefficients, $\\boldsymbol{\\beta}^{\\texttt{ridge}}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{\\texttt{ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\boldsymbol{I}$ is the $\\texttt{p√óp}$ identity matrix.\n",
    "\n",
    "We can see from this that for $\\boldsymbol{\\lambda}=0$ this model reduces to OLS. The bigger the value of $\\boldsymbol{\\lambda}$, the stricter the restriction on the size of the $\\boldsymbol{\\beta}$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression\n",
    "Like ridge regression, lasso (least absolute shrinkage and selection operator) regression adds a penalty to the loss function. We say lasso performs L1 regularization by adding a penalty equivalent to the absolute value of the magnitude of the coefficients.\n",
    "\n",
    "The loss function becomes \n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta})=\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\sum_{j=1}^{p}|\\beta_j|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling \n",
    "### The bootstrap\n",
    "### k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error measures\n",
    "The $r^2$ score, also known as the coefficient of determination, is a common measure of how well a model is able to predict outcomes. It is defined as one minus the residual sum of squares, \n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2\n",
    "$$ \n",
    "\n",
    "divided by the total sum of squares,\n",
    "\n",
    "$$\n",
    "\\text{TSS} = \\sum_{i=1}^{n}\\left(y_i-{y}_{mean}\\right)^2\n",
    "$$\n",
    "\n",
    "giving;\n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{RSS}{TSS}\n",
    "$$\n",
    "\n",
    "Here values closer to 1 are better, with $r^2=1.0$ being the optimal model. It is worth noting that $r^2$ can take negative values.\n",
    "\n",
    "The MSE is the mean of the square of the errors, or residual sum of squares:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    "and naturally values closer to zero are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Before a data analysis can begin is important to preprosess the data we are working on. This includes removing inconsistent and corrupted values, confirming completeness of the dataset and possibly removing highly corrolated features as well as outliers. It may also include selecting certain features from the dataset to focus on to reduce dimensionality.\n",
    "\n",
    "In this project our data is either synthetic and as such well defined, or we use map data where the issues mentioned are not relevant concerns. What is most relevant in this assignment is scaling.\n",
    "### Scaling\n",
    "Many models are sensitive to the effective value range of the features or input data. There are several ways to employ scaling. One popular option is to adjust the data so each predictor has mean value equal to zero and a variance of one. Another option is to scale all the data points so each feature vector has the same euclidian lentgh. Yet another option is to scale the values to all lie between a given minimum and maximum value, typically zero and one. \n",
    "### Dividing the data set\n",
    "A crucial step when trying to use regression to create a model based on a dataset is to divide up the dataset in at least two sets. This being a training set to train the model, and a test set to test it. In addition, if the size of the data set allows, one may also add a validation set for validating and fine tuning the model before tesing it on the test set. \n",
    "\n",
    "I will be dividing the data into training and test sets, and use cross validation in place of a separate validation test. I will be using a 75%/25% split between training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Tools\n",
    "While I have written my own code for the OLS and ridge regression models, as well as the bootstrap and kFold CV, I have used functionality from the library scikit-learn[3] for the lasso regression as well as for scaling and splitting the data set. This python library is based on numpy and and scipy, and contains a wide array of machine learning algorithms, including regression methods.\n",
    "\n",
    "Other packages I've used is numpy[4] for array handling, matplotlib.pyplot[5] for plots and visualizations, and python's random module[6] for generating (pseudo) random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "[1] USGSs (the United States Geological Survey) EarthExplorer tool - https://earthexplorer.usgs.gov/\n",
    "\n",
    "[2] Franke, R. (1979). A critical comparison of some methods for interpolation of scattered data\n",
    "\n",
    "[3] SciKit-learn: https://scikit-learn.org/stable/index.html\n",
    "\n",
    "[4] Numpy: https://numpy.org/\n",
    "\n",
    "[5] MatPlotLib.PyPlot: https://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "[6] Python.random: https://docs.python.org/3/library/random.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
