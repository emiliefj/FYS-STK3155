{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushrooms\n",
    "The mushroom dataset containes descriptive data for (hypothetical) samples of 23 species of gilled mushrooms in the Agaricus and Lepiota Family. The samples are drawn from *The Audubon Society Field Guide to North American Mushrooms* (1981). Each species is classified as either edible (class e) or poisonus (class p), where the poisonus category includes both species known to be poisonus as well as species where edibility is unknown.\n",
    "\n",
    "The data set contains a total of 8124 samples, each described with 22 descriptors. To reduce the size of the dataset, each attribute value is coded to a letter. These attributes are as follows:\n",
    "1. cap-shape:\n",
    "    * bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
    "2. cap-surface:\n",
    "    * fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "3. cap-color:\n",
    "    * brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\n",
    "4. bruises?:\n",
    "    * bruises=t,no=f\n",
    "5. odor:\n",
    "    * almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n",
    "6. gill-attachment:\n",
    "    * attached=a,descending=d,free=f,notched=n\n",
    "7. gill-spacing:\n",
    "    * close=c,crowded=w,distant=d\n",
    "8. gill-size:\n",
    "    * broad=b,narrow=n\n",
    "9. gill-color:\n",
    "    * black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "10. stalk-shape:\n",
    "    * enlarging=e,tapering=t\n",
    "11. stalk-root:\n",
    "    * bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "12. stalk-surface-above-ring:\n",
    "    * fibrous=f,scaly=y,silky=k,smooth=s\n",
    "13. stalk-surface-below-ring:\n",
    "    * fibrous=f,scaly=y,silky=k,smooth=s\n",
    "14. stalk-color-above-ring:\n",
    "    * brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "15. stalk-color-below-ring:\n",
    "    * brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "16. veil-type:\n",
    "    * partial=p,universal=u\n",
    "17. veil-color:\n",
    "    * brown=n,orange=o,white=w,yellow=y\n",
    "18. ring-number:\n",
    "    * none=n,one=o,two=t\n",
    "19. ring-type:\n",
    "    * cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "20. spore-print-color:\n",
    "    * black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "21. population:\n",
    "    * abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "22. habitat:\n",
    "    * grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "   \n",
    "### Known Simple Rules\n",
    "As this data set has been studied extensively, several more or less comples rules have bben found for deciding whether a given mushroom is edible or not. Particularly a set of four markedly simple rules have been found that together give a 100 % accuracy on classifying poisonous mushrooms []:\n",
    "\n",
    "* $\\texttt{P_1}$: odor=NOT(almond.OR.anise.OR.none)\n",
    "\t     120 poisonous cases missed, 98.52% accuracy\n",
    "* $\\texttt{P_2}$: spore-print-color=green\n",
    "\t     48 cases missed, 99.41% accuracy\n",
    "         \n",
    "* $\\texttt{P_3}$: odor=none.AND.stalk-surface-below-ring=scaly.AND.(stalk-color-above-ring=NOT.brown) \n",
    "\t     8 cases missed, 99.90% accuracy\n",
    "         \n",
    "* $\\texttt{P_4}$: habitat=leaves.AND.cap-color=white\n",
    "\t     0 cases missed, 100% accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "A decision tree is a type of supervised learning model that can be used for both regression and classification problems. They are named *trees* as their structure consists of a root node recursively split into nodes willowing \"branches\", ending in the en-nodes also known as \"leaves\". Each split is based on a decision for one of the features of the data. When using a decision tree for prediction we move down the tree with our input data, for each node determining if the relevant feature is below or above some threshold, or if it is true/false or some other binary divider. When we reach the end of the tree, one of the leaf nodes, this node tells us the resulting prediction.\n",
    "\n",
    "The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).\n",
    "\n",
    "Decision trees are popular models for real life problems as they preduce easily interpretable models that resemble human decision making. They do not require normalization of the inputs, and they can be used to model non-linear relationships.\n",
    "\n",
    "They are, however, prone to overfitting and generally do not provide the best predictive accuracy. Other challenges for decision trees are that small changes in the data may lead to a completely different tree structure, and unbalanced datasets with a target feature value that occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones. In addition features with many levels may be preferred over features with fewer levels as it is then easier to split the dataset such that the splits only contain pure target feature values. \n",
    "\n",
    "Many of these issues can be improved upon by using ensemble methods, methods that aggregate several decision trees. This generally comes at the cost of intepretability.\n",
    "\n",
    "Available algorithms for building a decision tree include ID3, C4.5 and CART. These algoriths typically use different criteria for how to perform splitting, ID3 uses information gain, C4.5 uses gain ratio, while CART uses the gini index.\n",
    "### CART Algorithm\n",
    "Originally the term Classification And Regression Tree (CART) was introduced by Breiman et al.[] as an umbrella term used for analysis of regression as well as classification trees. The CART algorithm is the most commonly used algorith for building decision trees. It is a non-parametric learning technique for building trees. \n",
    "\n",
    "Using the CART algorithm trees are constructed using a top-down approach. We start by looking at all the available training data, and selecting the split that minimizes the cost function. This is then the root node. Split is performed in the same way moving down thre tree until a stopping criteria is met.\n",
    "\n",
    "To decide on the best split, a measure of impurity, $G$, is used. For CART this is typically the Gini index, while other options include the information entropy, or the misclassification rate, see the following sections. \n",
    "\n",
    "We split the dataset into two subsets $a$ and $b$ using a single feature $k$ and a threshold $t_k$, by finding the pair $(k,t_k)$ giving the lowest impurity for the subsets according to the chosen impurity measure. \n",
    "This minimizes our cost function for this problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(k,t_k) = \\frac{m_{\\mathrm{a}}}{m}G_{\\mathrm{a}}+ \\frac{m_{\\mathrm{b}}}{m}G_{\\mathrm{b}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $G_{\\mathrm{a/b}}$ measures the impurity of each of the subsets, and $m_{\\mathrm{a/b}}$ is the number of instances in subset $a$ and subset $b$, respectively.\n",
    "\n",
    "There are several possible stopping criteria, like maximum depth of tree, all members of the node belonging to the same class, the impurity factor decreasing by less than some threshold for further splits, or that the minimum number of node members is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "The Gini index is also called the Gini impurity, and it measures the probability of a particular variable that is randomly chosen being wrongly classified. As such it takes values between 0 and 1. Another way to look at it is that it measures the lack of 'purity' of the variables. A node is pure if all its variables or members belong to one class.The gini index then takes the value 0. The more of the members of the node that belong to a different class, the more impure the node is. \n",
    "\n",
    "Denoting the fraction of observations (or members) of node/region $m$ being classified to a particular class $k$ as $p_{mk}$, the Gini index, $g$ can be defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g = \\sum_{k=1}^K p_{mk}(1-p_{mk}) = 1-\\sum_{k=1}^K p_{mk}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraction $p_{mk}$ can be calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{mk} = \\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a decision tree using CART with the Gini index as impurity measure we choose the attribute/feature with the smalles Gini index as the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Entropy is another measure for impurity. It is known from thermodynamics as a measure of disorder. In the classification case the entropy, or information entropy, is a measure for how much information we gain by knowing the value (or classification) of more features. \n",
    "\n",
    "The entropy, $s$, can be defined in terms of the fraction $p_{mk}$ defined in the section above, as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s = -\\sum_{k=1}^K p_{mk}\\log{p_{mk}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "[] Mushrooms data set: https://archive.ics.uci.edu/ml/datasets/Mushroom, downloaded 04.12.2020\n",
    "\n",
    "[] Guide to building a decision tree: https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/, visited 04.12.2020\n",
    "\n",
    "[] Decision tree learning and the CART algorithm: https://en.wikipedia.org/wiki/Decision_tree_learning, visited 04.12.2020\n",
    "\n",
    "[] Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984). Classification and regression trees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software. ISBN 978-0-412-04841-8.\n",
    "\n",
    "[] Logical rules for classifying mushrooms: logical rules for mushrooms: Duch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H, Extraction of crisp logical rules using constrained backpropagation networks -\n",
    "\tcomparison of two new approaches, in: Proc. of the European Symposium\n",
    "\ton Artificial Neural Networks (ESANN'97), Bruge, Belgium 16-18.4.1997,\n",
    "\tpp. xx-xx\n",
    "\n",
    "[] On pruning decision trees: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py, visited 03.12.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
