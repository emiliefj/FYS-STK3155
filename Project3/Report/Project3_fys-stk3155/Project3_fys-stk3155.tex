\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Project3\_fys-stk3155}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

    

    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

    

    \hypertarget{data}{%
\section{Data}\label{data}}

    \hypertarget{mushrooms}{%
\subsection{Mushrooms[1]}\label{mushrooms}}

The mushroom dataset contains descriptive data for (hypothetical)
samples of 23 species of gilled mushrooms in the Agaricus and Lepiota
Family. The samples are drawn from \emph{The Audubon Society Field Guide
to North American Mushrooms} (1981)[2]. Each species is classified as
either edible (class e) or poisonous (class p), where the poisonous
category includes both species known to be poisonous as well as those where edibility is unknown.

The data set contains a total of 8124 samples, each described with 22
descriptors. To reduce the size of the dataset, each attribute value is
coded to a letter. These attributes are as follows:
\begin{enumerate}
	\item cap-shape:\\
	bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
	\item cap-surface: \\
	fibrous=f,grooves=g,scaly=y,smooth=s
	\item cap-color: \\
	brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y
	\item bruises?: \\
	bruises=t,no=f
	\item odor: \\
	almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s
	\item gill-attachment: \\
	attached=a,descending=d,free=f,notched=n
	\item gill-spacing: \\
	close=c,crowded=w,distant=d
	\item gill-size: \\
	broad=b,narrow=n
	\item gill-color: \\
	black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y
	\item stalk-shape: \\
	 enlarging=e,tapering=t
	\item stalk-root: \\
	bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
	\item stalk-surface-above-ring: \\
	fibrous=f,scaly=y,silky=k,smooth=s
	\item stalk-surface-below-ring: \\
	fibrous=f,scaly=y,silky=k,smooth=s
	\item stalk-color-above-ring: \\
	brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
	\item stalk-color-below-ring: \\
	brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
	\item veil-type: \\
	partial=p,universal=u
	\item veil-color: \\
	brown=n,orange=o,white=w,yellow=y
	\item ring-number: \\
	none=n,one=o,two=t
	\item ring-type: \\
	cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z
	\item spore-print-color: \\
	black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y
	\item population: \\
	abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y
	\item habitat: \\
	 grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d
	
\end{enumerate}
  
    \hypertarget{known-simple-rules}{%
\subsubsection{Known Simple Rules}\label{known-simple-rules}}

As this data set has been studied extensively, several more or less
complex rules have been found for deciding whether a given mushroom is
edible or not. Particularly a set of four markedly simple rules have
been found that together give a 100 \% accuracy on classifying poisonous
mushrooms {[}1{]}:

\begin{itemize}
\item
  \(\texttt{P}_1\): $\texttt{odor=NOT(almond.OR.anise.OR.none)}$ \newline
  120 poisonous cases missed, $98.52\%$ accuracy
\item
  \(\texttt{P}_2\): $\texttt{spore-print-color=green}$ \newline
   48 cases missed, 99.41\%
  accuracy
\item
  \(\texttt{P}_3\): $\texttt{odor=none.AND.stalk-surface-below-ring=scaly.AND.(stalk-color-above-ring=NOT.brown)}$ \newline
  8 cases missed, 99.90\% accuracy
\item
  \(\texttt{P}_4\): $\texttt{habitat=leaves.AND.cap-color=white}$ \newline
  0 cases missed, 100\% accuracy
\end{itemize}

    \hypertarget{fetal-health}{%
\subsection{Fetal Health[3][4]}\label{fetal-health}}

A dataset of 2126 entries, each described by 22 features. There are no
missing or $\texttt{Null}$ values. Table number shows values for descriptive
statistics like min and max values, mean, and standard deviation. The
target value is \(\texttt{fetal\_health}\) which can take one of three
classes:
\begin{itemize}
	\item Normal
	\item Suspect
	\item Pathological
\end{itemize}
as determined by three expert obstetricians.

We see from figure number that, not unexpectedly, most entries are in
the \emph{Normal} class. As such the dataset is quite unbalanced.

    \hypertarget{methods}{%
\section{Methods}\label{methods}}

    \hypertarget{decision-trees}{%
\subsection{Decision Trees}\label{decision-trees}}

A decision tree is a type of supervised learning model that can be used
for both regression and classification problems. They are named
\emph{trees} as their structure consists of a root node recursively
split into nodes, or "branches", ending in the end-nodes also known as
"leaves". Each split of a node is based on a choice or decision for
one of the features of the data, like for instance "is
height\textgreater=2.0m?".

When using a decision tree for prediction we move down the tree with our
input data, for each node determining whether to move left or right down
the tree based on the value of the input data for the relevant
"decision feature" at that node. Is the value below or above some
threshold, or equal/not equal to some value? When we reach the end of
the tree, one of the leaf nodes, this node tells us the resulting
prediction.

Decision trees are popular models for real life problems as they produce
easily interpretable models that resemble human decision making. They do
not require normalization of the inputs, and they can be used to model
non-linear relationships.

They are, however, prone to over-fitting and generally do not provide the
best predictive accuracy. Other challenges for decision trees are that
small changes in the data may lead to a completely different tree
structure, and unbalanced datasets with a target feature value that
occur much more often than others may lead to biased trees since
the frequently occurring feature values are preferred over the less
frequently occurring ones. In addition, features with many levels may be
preferred over features with fewer levels as it is then easier to split
the dataset such that the splits only contain pure target feature
values.

Many of these issues can be improved upon by using ensemble methods,
methods that aggregate several decision trees. This generally comes at
the cost of interpretability.

Available algorithms for building a decision tree include ID3, C4.5 and
CART. These algorithms typically use different criteria for how to
perform splitting, ID3 uses information gain, C4.5 uses gain ratio,
while CART uses the gini index. 

    \hypertarget{cart-algorithm}{%
	\subsubsection{CART Algorithm}\label{cart-algorithm}}
 Originally the term Classification And Regression Tree (CART) was introduced by Breiman
et al.{[}9{]} as an umbrella term used for analysis of regression as well
as classification trees. The CART algorithm is the most commonly used
algorithm for building decision trees. It is a non-parametric learning
technique.

With the CART algorithm trees are constructed using a top-down approach.
We start by looking at all the available training data, and selecting
the split that minimizes the cost function. This is then the root node.
Split is performed in the same way moving down the tree until a stopping
criteria is met.

To decide on the best split, a measure of impurity, \(\texttt{G}\), is
used. For CART this is typically the Gini index, while other options
include the information entropy, or the miss-classification rate, see the
following sections.

At each node we split the dataset into two subsets \(a\) and \(b\) using
a single feature \(k\) and a threshold \(t_k\), by finding the pair
\((k,t_k)\) giving the lowest impurity for the subsets according to the
chosen impurity measure. This minimizes our cost function for this
problem,

    \[
C(k,t_k) = \frac{m_{\mathrm{a}}}{m}G_{\mathrm{a}}+ \frac{m_{\mathrm{b}}}{m}G_{\mathrm{b}},
\]

    where \(G_{\mathrm{a/b}}\) measures the impurity of each of the subsets,
and \(m_{\mathrm{a/b}}\) is the number of instances in subset \(a\) and
subset \(b\), respectively.

There are several possible stopping criteria, like maximum depth of
tree, all members of the node belonging to the same class, the impurity
factor decreasing by less than some threshold for further splits, or
that the minimum number of node members is reached.


\hypertarget{building-the-tree}{%
\subsubsection*{Building the Tree}\label{building-the-tree}} 

When building the tree we start with the root node and move recursively down the tree as indicated in the following pseudocode: {[}6{]}

\begin{verbatim}
def find_split(input_data, target):
    start_impurity = find_impurity(data, target)
    split_threshold, split_feature, split_impurity
    for each feature in input_data:
        for each unique_value in feature:
            threshold = value
            impurity = find_impurity(feature, threshold)
            if impurity is better than split_impurity:
                split_threshold = threshold
                split_impurity = impurity
                split_feature = feature
    split_node(split_feature, split_threshold, input_data, target)
\end{verbatim}

    \hypertarget{gini-index}{%
\subsubsection{Gini Index}\label{gini-index}}

The Gini index is also called the Gini impurity, and it measures the
probability of a particular variable that is randomly chosen being
wrongly classified. As such it takes values between 0 and 1. Another way
to look at it is that it measures the lack of 'purity' of the variables.
A node is pure if all its variables or members belong to one class. The
gini index then takes the value 0. The more of the members of the node
that belong to a different class, the more impure the node is.

Denoting the fraction of observations (or members) of node/region \(m\)
being classified to a particular class \(k\) as \(p_{mk}\), the Gini
index, \(g\) can be defined as

    \[
g = \sum_{k=1}^K p_{mk}(1-p_{mk}) = 1-\sum_{k=1}^K p_{mk}^2.
\]

    The fraction \(p_{mk}\) can be calculated as

    \[
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k).
\]

    When building a decision tree using CART with the Gini index as impurity
measure we choose the attribute/feature with the smallest Gini index as
the root node.

    \hypertarget{entropy}{%
\subsubsection{Entropy}\label{entropy}}

Entropy is another measure for impurity. It is known from thermodynamics
as a measure of disorder. In the classification case the entropy, or
information entropy, is a measure for how much information we gain by
knowing the value (or classification) of more features.

The entropy, \(s\), can be defined in terms of the fraction \(p_{mk}\)
defined in the section above, as

    \[
s = -\sum_{k=1}^K p_{mk}\log{p_{mk}}.
\]

    \hypertarget{ensemble-methods}{%
\subsection{Ensemble Methods}\label{ensemble-methods}}

Ensemble methods use a set, or ensemble, of so-called weak learners, and
use their combined predictive power to make predictions. While each
individual model in the ensemble may have a poor performance, say only
just above random guessing, the resulting ensemble may perform very
well. Ensemble methods can use any (weak) learner as its base learner, and even a combination of different ones. We will, however look exclusively at ensembles of decision trees.

An individual decision tree is prone to over-fitting and high
variance. The idea is that when averaging over many of these weak
learners the variance of each tree averages out, reducing the total
variance and thereby error of the resulting model. We will be exploring three kinds of ensemble methods where
decision trees are the base learner; bagging, boosting, and random
forests.

    \hypertarget{bagging}{%
\subsubsection{Bagging}\label{bagging}}

Bagging is a simple form of an ensemble method. A set of \(\texttt{N}\)
trees is built from the input data, with the twist that each tree is
built only on a subset of the total input data. The subset is chosen by
randomly sampling of the data, with substitution. In that way each tree
is built on a bootstrap sample of the original training data. This can
effectively reduce the variance of the model. This improved performance
comes at the expense of the interpretability of the model.

\begin{BVerbatim}
	Bagging Algorithm:
	With training data X and y, and N trees. 
	for i from 1 to N:
	    1. set X_ and y_ equal to a subset of X and Y, drawn from X and y with 
	       replacement
	    2. fit tree i to X_ and y_ 
	    3. store tree i for future prediction
\end{BVerbatim}


    \hypertarget{random-forests}{%
\subsubsection{Random Forests}\label{random-forests}}

Random forests take bagging one step further by adding randomness in
what features are available when fitting each tree to the data. In
essence, in addition to fitting to a random sample of the input or
training data, the fit is done using a random subset of the available
features. 

Often every tree will be dominated by one or more strong or
defining features, with every tree having the same root node. By only
looking at a random subset of the features per tree we increase the
randomness in the resulting tree ensemble, and hope to further reduce
the variance. We are essentially reducing the correlation between the
individual trees, and as such expect an improved reduction in variance
over bagging where the trees will often remain very correlated.

One typically looks at \(m \approx\sqrt{p}\) predictors for each tree,
with \(p\) being the total number of predictors in the dataset.
\\
\\
\begin{BVerbatim}
    Random Forest Algorithm: 
    With training data X and y, with X made up of F features, and
    N trees. 
    for i from 1 to N: 
        1. set X_ and y_ equal to a subset of X and Y, drawn with 
           replacement 
        2. draw f features randomly from the F features 
        3. fit tree i to X_[f] and y_ 
        4. store tree for future prediction
\end{BVerbatim}


    \hypertarget{adaptive-boosting}{%
\subsubsection{Adaptive Boosting}\label{adaptive-boosting}}

While the ensemble methods described until now can easily be performed
in parallel, adaptive boosting, or AdaBoost, uses the resulting
prediction from each subsequent fit to improve the fit in the next.
After each tree is fit to the data the algorithm makes note of what data
is miss-classified in the resulting model, and gives these data increased
weight in further model fitting. Optional features include performing a
similar weighing of the features used, where features leading to good
predictions are given more weight, or discarding individual  models if their accuracy
is below some level.

\begin{BVerbatim}
	AdaBoost Algorithm:
	With training data X and y, and N trees.
	First assign equal weight to each observation
	weights = np.ones()*1./X.shape[0]
	for i from 1 to N:
	    1. fit tree i to the data, weighing the data according to weights
		2. calculate error by summing up the weight of misclassified
		   observations
	       error = sum(weights of misclassified observation)/sum(weights)
		3. update weights using the quantity
		   alpha=log(1 - error)/error: 
		   weight_i = weight_i*exp(alpha) if incorrectly classified 
		   weight_i = weight_i*exp(-alpha) if correctly classified 
\end{BVerbatim}

    \hypertarget{data-processing}{%
\subsection{Data Processing}\label{data-processing}}
Our datasets in this projects are fairly clean and well-defined already, and do not require much pre-processing. As we are working with decision trees we do not need to normalize our input data, it will not affect the results. As we are using datasets with fairly high dimensionality, we explore some methods of feature selection to see if we can reduce dimensionality without reducing model performance

\hypertarget{variance-threshold}{%
\subsubsection*{Variance Threshold}\label{variance-threshold}}

Feature selection by variance threshold is a simple form of feature selection where only features with variance above some threshold are kept. The logic behind this method is that the variance of a features signifies its spread. A feature that is more spread out across the entries will make it easier to separate entries based on this feature. In contrast, imagine if a feature takes only one value. This feature will then provide no information we can use to separate the entries. The strictness of this method is determined by the variance threshold used.


\hypertarget{univariate-feature-selection}{%
\subsubsection*{Univariate Feature Selection
{[}8{]}[12]}\label{univariate-feature-selection}}

In univariate feature selection features are selected based on their value according to some set scoring function. While the variance threshold can be used for any problem, we must in univariate feature selection select a scoring function suitable for our needs. We are only looking at classification problems, and our mushroom dataset consists solely of categorical input features. This limits the suitable scoring functions. 

We have used the \(\chi^2\) test statistic, which measures the dependence
between stochastic variables, and can be used for categorical features. The \(\chi^2\) test statistic is a statistical hypothesis test and uses the assumption that the observed frequencies for each categorical variable
match its expected frequencies. With this scoring function the features that are the most likely to be independent of the class and therefore irrelevant for classification are scored low, and can be removed. 

Our fetal health dataset on the other hand has continuous features, meaning the \(\chi^2\) test statistic is not suitable. In place of it we have used the ANOVA F-value. The ANOVA (Analysis of variance) f-value uses the f test statistic to evaluate each individual feature's ability to distinguish the classes of the variables. 

For both datasets we also use the mutual information. The mutual information measures the dependency between two random variables, with lower variables indicating more independent variables.


    \hypertarget{performance-measures}{%
\subsection{Performance Measures}\label{performance-measures}}

\hypertarget{accuracy}{%
\subsubsection{Accuracy}\label{accuracy}}
See Methods section in project 2[13].
\hypertarget{confusion-matrix}{%
\subsubsection{Confusion Matrix}\label{confusion-matrix}}
A confusion matrix is a useful tool for visualizing the predictive accuracy of a model. Unlike the measure above, the confusion matrix illustrates the predictive accuracy per class. This way we can see what classes the model performs well of less well for, and information like what class misclassifications en up in instead.

The matrix consists of one row and one column per class. The rows represent the predicted class, while the columns represent the actual or true class (or vice versa). Where the row and column for a class meet we have the number of correctly classified instances. Where rows and columns of different classes meet we have a wrongly classified instance, and we can see trends in misclassifications. See for instance figure number for an example. 

\hypertarget{roc-curve}{%
\subsubsection{ROC Curve}\label{roc-curve}}
A receiver operating characteristic curve, or ROC curve, is another tool for visualizing a model's predictive performance, more specifically a binary model. It consists of a graphical plot of false positive rate vs true positive rate for a set of discriminative thresholds. The area under the curve is one measure of model performance, with a perfect model getting a score of one.

\hypertarget{results-and-discussion}{%
\section{Results and Discussion}\label{results-and-discussion}}

    \hypertarget{pre-processing-the-data}{%
\subsection{Pre-processing the Data}\label{pre-processing-the-data}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../Code}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{DecisionTree} \PY{k}{as} \PY{n+nn}{dt}

\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../Data/Mushroom/UCI MLR/agaricus\PYZhy{}lepiota.data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                 \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cap\PYZhy{}shape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cap\PYZhy{}surface}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cap\PYZhy{}color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bruises}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{odor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gill\PYZhy{}attachment}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gill\PYZhy{}spacing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gill\PYZhy{}size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gill\PYZhy{}color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}shape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}root}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}surface\PYZhy{}above\PYZhy{}ring}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}surface\PYZhy{}below\PYZhy{}ring}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}color\PYZhy{}above\PYZhy{}ring}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stalk\PYZhy{}color\PYZhy{}below\PYZhy{}ring}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{veil\PYZhy{}type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{veil\PYZhy{}color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ring\PYZhy{}number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ring\PYZhy{}type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spore\PYZhy{}print\PYZhy{}color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{population}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{habitat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
  class cap-shape cap-surface cap-color bruises odor gill-attachment  \textbackslash{}
0     p         x           s         n       t    p               f
1     e         x           s         y       t    a               f
2     e         b           s         w       t    l               f
3     p         x           y         w       t    p               f
4     e         x           s         g       f    n               f

  gill-spacing gill-size gill-color  {\ldots} stalk-surface-below-ring  \textbackslash{}
0            c         n          k  {\ldots}                        s
1            c         b          k  {\ldots}                        s
2            c         b          n  {\ldots}                        s
3            c         n          n  {\ldots}                        s
4            w         b          k  {\ldots}                        s

  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \textbackslash{}
0                      w                      w         p          w
1                      w                      w         p          w
2                      w                      w         p          w
3                      w                      w         p          w
4                      w                      w         p          w

  ring-number ring-type spore-print-color population habitat
0           o         p                 k          s       u
1           o         p                 n          n       g
2           o         p                 n          n       m
3           o         p                 k          s       u
4           o         e                 n          a       g

[5 rows x 23 columns]
\end{Verbatim}
\end{tcolorbox}
        
    As we know, all the features of this dataset are categorical. We can
first explore the data by looking at the unique values for each feature,
see table number.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Table 1: Unique values of the features in the mushroom data set.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{unique values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{df}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{unique}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} unique values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{df}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Table 1: Unique values of the features in the mushroom data set.
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                                                 unique values  \textbackslash{}
class                                                   [p, e]
cap-shape                                   [x, b, s, f, k, c]
cap-surface                                       [s, y, f, g]
cap-color                       [n, y, w, g, e, p, b, u, c, r]
bruises                                                 [t, f]
odor                               [p, a, l, n, f, c, y, s, m]
gill-attachment                                         [f, a]
gill-spacing                                            [c, w]
gill-size                                               [n, b]
gill-color                [k, n, g, p, w, h, u, e, b, r, y, o]
stalk-shape                                             [e, t]
stalk-root                                     [e, c, b, r, ?]
stalk-surface-above-ring                          [s, f, k, y]
stalk-surface-below-ring                          [s, f, y, k]
stalk-color-above-ring             [w, g, p, n, b, e, o, c, y]
stalk-color-below-ring             [w, p, g, b, n, e, y, o, c]
veil-type                                                  [p]
veil-color                                        [w, n, o, y]
ring-number                                          [o, t, n]
ring-type                                      [p, e, l, f, n]
spore-print-color                  [k, n, u, h, w, r, o, y, b]
population                                  [s, n, a, v, y, c]
habitat                                  [u, g, m, d, p, w, l]

                          \# unique values
class                                   2
cap-shape                               6
cap-surface                             4
cap-color                              10
bruises                                 2
odor                                    9
gill-attachment                         2
gill-spacing                            2
gill-size                               2
gill-color                             12
stalk-shape                             2
stalk-root                              5
stalk-surface-above-ring                4
stalk-surface-below-ring                4
stalk-color-above-ring                  9
stalk-color-below-ring                  9
veil-type                               1
veil-color                              4
ring-number                             3
ring-type                               5
spore-print-color                       9
population                              6
habitat                                 7
\end{Verbatim}
\end{tcolorbox}
        
    As we can see, there is only one used value for veil-type, `p' or
partial. This feature then provides us with no information that we can
use to distinguish the different mushrooms, and we remove the feature
completely. We also check to make sure none of the entries are missing
values.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{veil\PYZhy{}type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Table 2: The number of missing values for each features. We see that no values are missing.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{df}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Table 2: The number of missing values for each features. We see that no values
are missing.
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
class                       0
cap-shape                   0
cap-surface                 0
cap-color                   0
bruises                     0
odor                        0
gill-attachment             0
gill-spacing                0
gill-size                   0
gill-color                  0
stalk-shape                 0
stalk-root                  0
stalk-surface-above-ring    0
stalk-surface-below-ring    0
stalk-color-above-ring      0
stalk-color-below-ring      0
veil-color                  0
ring-number                 0
ring-type                   0
spore-print-color           0
population                  0
habitat                     0
dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    Another point of interest is whether the data are fairly evenly divided
amongst the two categories. The check this by comparing the number of
entries belonging to each class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Creating histogram }
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{n}{s} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{class}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data} \PY{o}{=} \PY{n}{df}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Show plot }
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Figure 2: Histogram of class distribution. We can see the dataset contains a fairly even split between the two}\PY{l+s+se}{\PYZbs{}}
\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{classes.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Figure 2: Histogram of class distribution. We can see the dataset contains a
fairly even split between the two
classes.
    \end{Verbatim}

    Figure number shows a heatmat of the correlations between the features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{92}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Now we have a look at the correlation matrix}
\PY{n}{df\PYZus{}numerical} \PY{o}{=} \PY{n}{df}
\PY{c+c1}{\PYZsh{} First we need to map to number\PYZhy{}values}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{LabelEncoder}
\PY{n}{les} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{col}\PY{p}{:} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{df\PYZus{}numerical}\PY{o}{.}\PY{n}{columns}\PY{p}{\PYZcb{}}
\PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{les}\PY{p}{:}
    \PY{n}{df\PYZus{}numerical}\PY{p}{[}\PY{n}{col}\PY{p}{]} \PY{o}{=} \PY{n}{les}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df\PYZus{}numerical}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{p}{)}

\PY{n}{df\PYZus{}numerical}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{92}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   class  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \textbackslash{}
0      1          5            2          4        1     6                1
1      0          5            2          9        1     0                1
2      0          0            2          8        1     3                1
3      1          5            3          8        1     6                1
4      0          5            2          3        0     5                1

   gill-spacing  gill-size  gill-color  {\ldots}  stalk-surface-above-ring  \textbackslash{}
0             0          1           4  {\ldots}                         2
1             0          0           4  {\ldots}                         2
2             0          0           5  {\ldots}                         2
3             0          1           5  {\ldots}                         2
4             1          0           4  {\ldots}                         2

   stalk-surface-below-ring  stalk-color-above-ring  stalk-color-below-ring  \textbackslash{}
0                         2                       7                       7
1                         2                       7                       7
2                         2                       7                       7
3                         2                       7                       7
4                         2                       7                       7

   veil-color  ring-number  ring-type  spore-print-color  population  habitat
0           2            1          4                  2           3        5
1           2            1          4                  3           2        1
2           2            1          4                  3           2        3
3           2            1          4                  2           3        5
4           2            1          0                  3           0        1

[5 rows x 22 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{93}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{correlation\PYZus{}matrix} \PY{o}{=} \PY{n}{df\PYZus{}numerical}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)} 
\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{correlation\PYZus{}matrix}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{vmin}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Figure number: A heatmap showing the correlation matrix for the features in the mushroom dataset.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Figure number: A heatmap showing the correlation matrix for the features in the
mushroom dataset.
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see that \(\texttt{class}\) is most highly correlated with
\(\texttt{gill-size}\), \(\texttt{gill-color}\), and
\(\texttt{bruises}\), followed by \(\texttt{ring-type}\),
\(\texttt{stalk-root}\), and \(\texttt{gill-spacing}\). The most highly
corrolated features (meaning they provide much of the same information)
are \(\texttt{gill-attachment}\) and \(\texttt{ring-number}\), with a
correlation of 0.9. Other fairly correlated features are
\(\texttt{bruises}\) and \(\texttt{ring-type}\), \(\texttt{gill-color}\)
and \(\texttt{ring-type}\), and \(\texttt{gill-size}\) and
\(\texttt{spore-print-color}\).

    \hypertarget{feature-selection}{%
\subsection{Feature Selection}\label{feature-selection}}

As we have a fairly high-dimensional problem we performed feature
selection in order to reduce the number of features. The goal for this
was both to increase speed of fitting a model to the data, and to
hopefully end up with a simpler model that is easier to interpret,
understand, and use.

We have tried three methods for this; a simple variance threshold where
features with a variance below a certain threshold or cutoff value are
excluded, as well as univariate feature selection using two different
scoring functions.

We use a cutoff value of 0.8 as variance threshold, which results in
excluding four of the features, namely \(\texttt{gill-attachment}\),
\(\texttt{gill-spacing}\), \(\texttt{veil-color}\), and
\(\texttt{ring-number}\). The number of features is then reduced from 21
to 17. A plot of the variances for the different features is shown in
figure number.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} train \PYZhy{} validaton \PYZhy{} test \PYZhy{} splitting 60 \PYZhy{} 20 \PYZhy{} 20}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{n}{seed} \PY{o}{=} \PY{l+m+mi}{68}
\PY{n}{train\PYZus{}frac} \PY{o}{=} \PY{l+m+mf}{0.6}  \PY{c+c1}{\PYZsh{} out of the whole set}
\PY{n}{test\PYZus{}frac} \PY{o}{=} \PY{l+m+mf}{0.5}   \PY{c+c1}{\PYZsh{} out of the non\PYZhy{}training set}

\PY{n}{y} \PY{o}{=} \PY{n}{df\PYZus{}numerical}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{X} \PY{o}{=} \PY{n}{df\PYZus{}numerical}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}


\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{train\PYZus{}frac}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{seed}\PY{p}{)}
\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{test\PYZus{}frac}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{seed}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{}train, validate, test = np.split(df.sample(frac=1, random\PYZus{}state=42), [int(.6*len(df)), int(.8*len(df))])}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(1625, 21)
\end{Verbatim}
\end{tcolorbox}
        
    We can compare this to feature selection by comparing the value of
certain scoring functions for each of the features. The result using
chi-squared statistic, can be seen in figure number. We find from this
result that comparing the features using this statistic the key features
to include are features 8, 17, 7, 3, 10, 20, and 6, corresponding to
\(\texttt{gill-color}\), \(\texttt{ring-type}\), \(\texttt{gill-size}\),
\(\texttt{bruises}\), \(\texttt{stalk-root}\), \(\texttt{habitat}\), and
\(\texttt{gill-spacing}\).

Using instead the mutual information as scoring function we find that
more features are included, Most notably feature four,
\(\texttt{odor}\), has gone from not being included to being the main
feature. A bar plot is shown in figure number. This result correlates
more closely with our expectations as we know from the known rules
described in the Data section that using only \(\texttt{odor}\) is
enough to get a \(98.52\%\) accuracy on this dataset.

    Figure number: Bar plot showing the chi-squared scores of the the
features in the mushroom dataset. We see that feature eight has the
clear highest score. This corresponds to \(\texttt{gill-color}\).

    Figure number: Bar plot showing the mutual information scores of the the
features in the mushroom dataset. We see that feature four,
\(\texttt{odor}\) scores highest.

    \hypertarget{fetal-health}{%
\subsection{Fetal Health}\label{fetal-health}}


    \begin{Verbatim}[commandchars=\\\{\}]
Table number: A table showing key statistical values for the features in the
fetal health dataset.
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                                                     count        mean  \textbackslash{}
baseline value                                      2126.0  133.303857
accelerations                                       2126.0    0.003178
fetal\_movement                                      2126.0    0.009481
uterine\_contractions                                2126.0    0.004366
light\_decelerations                                 2126.0    0.001889
severe\_decelerations                                2126.0    0.000003
prolongued\_decelerations                            2126.0    0.000159
abnormal\_short\_term\_variability                     2126.0   46.990122
mean\_value\_of\_short\_term\_variability                2126.0    1.332785
percentage\_of\_time\_with\_abnormal\_long\_term\_vari{\ldots}  2126.0    9.846660
mean\_value\_of\_long\_term\_variability                 2126.0    8.187629
histogram\_width                                     2126.0   70.445908
histogram\_min                                       2126.0   93.579492
histogram\_max                                       2126.0  164.025400
histogram\_number\_of\_peaks                           2126.0    4.068203
histogram\_number\_of\_zeroes                          2126.0    0.323612
histogram\_mode                                      2126.0  137.452023
histogram\_mean                                      2126.0  134.610536
histogram\_median                                    2126.0  138.090310
histogram\_variance                                  2126.0   18.808090
histogram\_tendency                                  2126.0    0.320320
fetal\_health                                        2126.0    1.304327

                                                          std    min      25\%  \textbackslash{}
baseline value                                       9.840844  106.0  126.000
accelerations                                        0.003866    0.0    0.000
fetal\_movement                                       0.046666    0.0    0.000
uterine\_contractions                                 0.002946    0.0    0.002
light\_decelerations                                  0.002960    0.0    0.000
severe\_decelerations                                 0.000057    0.0    0.000
prolongued\_decelerations                             0.000590    0.0    0.000
abnormal\_short\_term\_variability                     17.192814   12.0   32.000
mean\_value\_of\_short\_term\_variability                 0.883241    0.2    0.700
percentage\_of\_time\_with\_abnormal\_long\_term\_vari{\ldots}  18.396880    0.0    0.000
mean\_value\_of\_long\_term\_variability                  5.628247    0.0    4.600
histogram\_width                                     38.955693    3.0   37.000
histogram\_min                                       29.560212   50.0   67.000
histogram\_max                                       17.944183  122.0  152.000
histogram\_number\_of\_peaks                            2.949386    0.0    2.000
histogram\_number\_of\_zeroes                           0.706059    0.0    0.000
histogram\_mode                                      16.381289   60.0  129.000
histogram\_mean                                      15.593596   73.0  125.000
histogram\_median                                    14.466589   77.0  129.000
histogram\_variance                                  28.977636    0.0    2.000
histogram\_tendency                                   0.610829   -1.0    0.000
fetal\_health                                         0.614377    1.0    1.000

                                                        50\%      75\%      max
baseline value                                      133.000  140.000  160.000
accelerations                                         0.002    0.006    0.019
fetal\_movement                                        0.000    0.003    0.481
uterine\_contractions                                  0.004    0.007    0.015
light\_decelerations                                   0.000    0.003    0.015
severe\_decelerations                                  0.000    0.000    0.001
prolongued\_decelerations                              0.000    0.000    0.005
abnormal\_short\_term\_variability                      49.000   61.000   87.000
mean\_value\_of\_short\_term\_variability                  1.200    1.700    7.000
percentage\_of\_time\_with\_abnormal\_long\_term\_vari{\ldots}    0.000   11.000   91.000
mean\_value\_of\_long\_term\_variability                   7.400   10.800   50.700
histogram\_width                                      67.500  100.000  180.000
histogram\_min                                        93.000  120.000  159.000
histogram\_max                                       162.000  174.000  238.000
histogram\_number\_of\_peaks                             3.000    6.000   18.000
histogram\_number\_of\_zeroes                            0.000    0.000   10.000
histogram\_mode                                      139.000  148.000  187.000
histogram\_mean                                      136.000  145.000  182.000
histogram\_median                                    139.000  148.000  186.000
histogram\_variance                                    7.000   24.000  269.000
histogram\_tendency                                    0.000    1.000    1.000
fetal\_health                                          1.000    1.000    3.000
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Figure number: A pie plot showing how the target variable fetal health is
distributed among the three possible classes.
    \end{Verbatim}

    \begin{center}
    \adjustimage{width=1.2\textwidth,center}{output_49_1.png}
    \end{center}
    { \hspace*{\fill} \\}

 \begin{Verbatim}[commandchars=\\\{\}]
 	Figure number: A heatmap showing the correlation matrix for the features 
 	in the fetal health dataset.
 \end{Verbatim}
 

%    \begin{Verbatim}[commandchars=\\\{\}]
Table number: Focusing on the correlations between the features and the target, 
fetal health. We see that $\texttt{prolongued\_decelerations}$, 
$\texttt{abnormal\_short\_term\_variability}$, and 
$\texttt{percentage\_of\_time\_with\_abnormal\_long\_term\_variability}$ are most closely 
correlated with the resulting fetal health classification. Note that the absolute 
values of the correlations are shown, in descending order.
%    \end{Verbatim}

    \begin{center}
	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{ordered_correlations_fetal_health.png}
\end{center}
{ \hspace*{\fill} \\}
        
    Figure number shows the correlation matrix for this dataset, while table
number makes the correlation of the features to the target more clear.
We see that \(\texttt{prolongued\_decelerations}\),
\(\texttt{abnormal\_short\_term\_variability}\), and
\(\texttt{percentage\_of\_time\_with\_abnormal\_long\_term\_variability}\) are
the features that correlate most strongly witht he resulting
classification. Between the features we see that there is naturally a
very strong correlation between \(\texttt{histogram\_mode}\),
\(\texttt{histogram\_mean}\), and \(\texttt{histogram\_median}\). as well
as between \(\texttt{histogram\_width}\), \(\texttt{histogram\_min}\), and
\(\texttt{histogram\_max}\). Particularly for the mode, mean and median
we see that they correlate in a very similar manner to all the other
features as well and as such contribute with mush the same information,
which means we do not gain much, if any, more insight by including all
three in our model.

 
    \begin{Verbatim}[commandchars=\\\{\}]

* Created a decision tree with 6 leaves, and a depth of 5 at the deepest.*

Testing own decision tree code on fetal\_health dataset:
Train accuracy:  0.8294117647058824
Test accuracy:  0.8352941176470589

The tree using my own code:
|--- histogram\_mean <= 89.00
|   |--- weights:  class: 3.0,    prediction: ['0.0', '0.0', '1.0']
|--- histogram\_mean >  89.00
|   |--- mean\_value\_of\_long\_term\_variability <= 73.00
|   |   |--- mean\_value\_of\_short\_term\_variability <= 81.00
|   |   |   |--- histogram\_median <= 87.00
|   |   |   |   |--- weights:  class: 3.0,    prediction: ['0.0', '0.0', '1.0']
|   |   |   |--- histogram\_median >  87.00
|   |   |   |   |--- abnormal\_short\_term\_variability <= 0.00
|   |   |   |   |   |--- weights:  class: 1.0,    prediction: ['0.8', '0.1',
'0.0']
|   |   |   |   |--- abnormal\_short\_term\_variability >  0.00
|   |   |   |   |   |--- weights:  class: 3.0,    prediction: ['0.0', '0.0',
'1.0']
|   |   |--- mean\_value\_of\_short\_term\_variability >  81.00
|   |   |   |--- weights:  class: 3.0,    prediction: ['0.0', '0.0', '1.0']
|   |--- mean\_value\_of\_long\_term\_variability >  73.00
|   |   |--- weights:  class: 3.0,    prediction: ['0.0', '0.0', '1.0']



Testing scikit-learn's DecisionTreeClassifier on fetal\_health dataset:
Train accuracy:  0.941764705882353
Test accuracy:  0.9458823529411765
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]

The tree using scikitlearn's DecisionTreeClassifier:
|--- percentage\_of\_time\_with\_abnormal\_long\_term\_variability <= 0.55
|   |--- mean\_value\_of\_long\_term\_variability <= 68.50
|   |   |--- mean\_value\_of\_short\_term\_variability <= 59.50
|   |   |   |--- histogram\_max <= 138.50
|   |   |   |   |--- uterine\_contractions <= 0.02
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- uterine\_contractions >  0.02
|   |   |   |   |   |--- class: 2.0
|   |   |   |--- histogram\_max >  138.50
|   |   |   |   |--- histogram\_min <= 17.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- histogram\_min >  17.50
|   |   |   |   |   |--- class: 2.0
|   |   |--- mean\_value\_of\_short\_term\_variability >  59.50
|   |   |   |--- mean\_value\_of\_short\_term\_variability <= 79.50
|   |   |   |   |--- mean\_value\_of\_long\_term\_variability <= 6.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- mean\_value\_of\_long\_term\_variability >  6.50
|   |   |   |   |   |--- class: 2.0
|   |   |   |--- mean\_value\_of\_short\_term\_variability >  79.50
|   |   |   |   |--- light\_decelerations <= 0.00
|   |   |   |   |   |--- class: 3.0
|   |   |   |   |--- light\_decelerations >  0.00
|   |   |   |   |   |--- class: 1.0
|   |--- mean\_value\_of\_long\_term\_variability >  68.50
|   |   |--- light\_decelerations <= 0.00
|   |   |   |--- class: 3.0
|   |   |--- light\_decelerations >  0.00
|   |   |   |--- mean\_value\_of\_long\_term\_variability <= 77.00
|   |   |   |   |--- class: 1.0
|   |   |   |--- mean\_value\_of\_long\_term\_variability >  77.00
|   |   |   |   |--- class: 3.0
|--- percentage\_of\_time\_with\_abnormal\_long\_term\_variability >  0.55
|   |--- histogram\_median <= 107.50
|   |   |--- histogram\_number\_of\_peaks <= 220.50
|   |   |   |--- mean\_value\_of\_short\_term\_variability <= 25.00
|   |   |   |   |--- histogram\_median <= 102.00
|   |   |   |   |   |--- class: 2.0
|   |   |   |   |--- histogram\_median >  102.00
|   |   |   |   |   |--- class: 1.0
|   |   |   |--- mean\_value\_of\_short\_term\_variability >  25.00
|   |   |   |   |--- class: 3.0
|   |   |--- histogram\_number\_of\_peaks >  220.50
|   |   |   |--- histogram\_tendency <= 23.50
|   |   |   |   |--- class: 3.0
|   |   |   |--- histogram\_tendency >  23.50
|   |   |   |   |--- class: 1.0
|   |--- histogram\_median >  107.50
|   |   |--- mean\_value\_of\_long\_term\_variability <= 6.50
|   |   |   |--- abnormal\_short\_term\_variability <= 0.00
|   |   |   |   |--- histogram\_max <= 134.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- histogram\_max >  134.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |--- abnormal\_short\_term\_variability >  0.00
|   |   |   |   |--- accelerations <= 129.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- accelerations >  129.50
|   |   |   |   |   |--- class: 3.0
|   |   |--- mean\_value\_of\_long\_term\_variability >  6.50
|   |   |   |--- histogram\_number\_of\_peaks <= 185.50
|   |   |   |   |--- histogram\_median <= 143.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- histogram\_median >  143.50
|   |   |   |   |   |--- class: 1.0
|   |   |   |--- histogram\_number\_of\_peaks >  185.50
|   |   |   |   |--- mean\_value\_of\_short\_term\_variability <= 49.00
|   |   |   |   |   |--- class: 1.0
|   |   |   |   |--- mean\_value\_of\_short\_term\_variability >  49.00
|   |   |   |   |   |--- class: 2.0

    \end{Verbatim}

   
    

    

    \hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

    

    \hypertarget{bibliography}{%
\section{Bibliography}\label{bibliography}}

{[}1{]} Mushrooms data set:
https://archive.ics.uci.edu/ml/datasets/Mushroom, downloaded 04.12.2020

{[}2{]} Source mushroom data: Mushroom records drawn from The Audubon
Society Field Guide to North American Mushrooms (1981). G. H. Lincoff
(Pres.), New York: Alfred A. Knopf

{[}3{]} Fetal health data set:
https://www.kaggle.com/andrewmvd/fetal-health-classification, downloaded
11.12.2020

{[}4{]} Source Fetal health data: Ayres de Campos et al.~(2000) SisPorto
2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal
Med 5:311-318 (link)

{[}5{]} Fetal health walkthrough:
https://www.kaggle.com/pariaagharabi/step-by-step-fetal-health-prediction-99-detailed

{[}6{]} Guide to building a decision tree:
https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/,
visited 04.12.2020

{[}7{]} Decision tree learning and the CART algorithm:
https://en.wikipedia.org/wiki/Decision\_tree\_learning, visited
04.12.2020

{[}8{]} Feature selection with scikit-learn:
https://scikit-learn.org/stable/modules/feature\_selection.html\#univariate-feature-selection,
visited 08.12.2020

{[}9{]} Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J.
(1984). Classification and regression trees. Monterey, CA: Wadsworth \&
Brooks/Cole Advanced Books \& Software. ISBN 978-0-412-04841-8.

{[}10{]} Logical rules for classifying mushrooms: logical rules for
mushrooms: Duch W, Adamczak R, Grabczewski K, Ishikawa M, Ueda H,
Extraction of crisp logical rules using constrained backpropagation
networks - comparison of two new approaches, in: Proc. of the European
Symposium on Artificial Neural Networks (ESANN'97), Bruge, Belgium
16-18.4.1997, pp.~xx-xx

{[}11{]} On pruning decision trees:
https://scikit-learn.org/stable/auto\_examples/tree/plot\_cost\_complexity\_pruning.html\#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py,
visited 03.12.2020

{[}12{]} Decision tree code:
https://medium.com/datadriveninvestor/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae,
visited 06.12.2020

{[}12{]} Guide on feature selection for categorical variables:
https://machinelearningmastery.com/feature-selection-with-categorical-data/,
visited 08.12.2020

[13] Project 2: https://github.com/emiliefj/FYS-STK3155/blob/master/Project2/Report/Project%202%20-%20FYS-STK3155.pdf


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
